# ğŸ§  RAG Pipeline with ChromaDB, FastAPI & LLM Agents

This project implements a complete **Retrieval-Augmented Generation (RAG)** backend using:

* **FastAPI** â€” API server for ingestion, querying, and LLM responses
* **ChromaDB** â€” vector store for embeddings + metadata
* **Custom LLM Agents** â€” metadata extraction & retrieval QA
* **PDF/Text ingestion pipeline** â€” chunking, embedding, and metadata generation

It is designed to be **modular, production-ready, and optimized for Persian/Farsi documents**.

---

# Why ChromaDB?

### ğŸ”¹ 1. High Performance in Similarity Search & Ingestion

* Uses **HNSW**, one of the fastest ANN algorithms
* Millisecond-level similarity queries
* Efficient ingestion & indexing, even locally
* Fully supports **Persian/Farsi embeddings**

### ğŸ”¹ 2. Scalable & Easy to Deploy

Chroma supports:

* Local embedded mode
* Persistent on-disk mode
* Server mode for distributed or cloud setups

This allows you to:

* Start small
* Scale to Docker/Kubernetes
* Migrate to Milvus/Weaviate later if needed

### ğŸ”¹ 3. Excellent Tooling & Community Support

* First-class integration with **LangChain**
* Active community + stable development
* Simple debugging thanks to SQLite storage
* Works well with FastAPI, LlamaIndex, HuggingFace, etc.

---

# ğŸ“ Project Structure

```
src/
â”œâ”€â”€ agents/               # LLM-powered agents (retrieval QA + metadata extractor)
â”œâ”€â”€ api/                  # FastAPI routes + schemas
â”œâ”€â”€ config/               # Project-wide configuration
â”œâ”€â”€ data_types/           # Pydantic models and custom types
â”œâ”€â”€ db/                   # ChromaDB wrapper + persisted database
â”œâ”€â”€ functions/            # Embedding, file loading, and chunk splitting
â”œâ”€â”€ llm/                  # LLM clients + prompt templates
â”œâ”€â”€ rag/                  # Ingestion and retrieval pipelines
â””â”€â”€ main.py               # FastAPI entrypoint

static/data/              # Example TXT/PDF documents
```

Each folder has a clear responsibility, making the system maintainable and extensible.

---

# ğŸš€ Features

## âœ… 1. Full RAG Pipeline

Automatically:

* Loads text and PDF files
* Splits into semantic chunks
* Generates embeddings
* Extracts structured metadata
* Saves everything into ChromaDB

## âœ… 2. Retrieval QA Agent

`retrieval_qa.py`:

* Retrieves top-K relevant chunks
* Builds a RAG-optimized prompt
* Uses an LLM to answer accurately
* Returns **answer + sources + similarity scores**

## âœ… 3. Metadata Extraction Agent

Each chunk is enriched with:

* Document ID
* Source filepath
* Title
* Date

### Why This Schema Improves RAG Quality

**Better filtering**
â†’ query only relevant documents or time ranges

**Faster retrieval**
â†’ smaller chunks = faster indexing + better similarity search

**Higher accuracy / fewer hallucinations**
â†’ LLM sees only the correct, context-aligned chunk content

This results in **better grounding and higher answer quality**.

## âœ… 4. FastAPI Backend

### Endpoints

#### **`POST /ingest`**

Upload TXT/PDF â†’ extract â†’ chunk â†’ embed â†’ store

#### **`POST /query`**

Send a question â†’ retrieve chunks â†’ LLM answer

### Example Request

```json
{
  "query": "Ø¯Ø®Ø§Ù†ÛŒØ§Øª Ø¨Ø±Ø§ÛŒ Ú†Ù‡ Ø³Ù†ÛŒ Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³ØªØŸ",
  "k": 5
}
```

### Example Response

```json
{
  "answer": "Ø®Ø±ÛŒØ¯ØŒ ÙØ±ÙˆØ´ Ùˆ Ù…ØµØ±Ù Ù‡Ø±Ú¯ÙˆÙ†Ù‡ Ù…Ø­ØµÙˆÙ„ Ø¯Ø®Ø§Ù†ÛŒØ§Øª Ø¨Ø±Ø§ÛŒ Ø§ÙØ±Ø§Ø¯ÛŒ Ú©Ù‡ Ø¯Ø± ØªØ§Ø±ÛŒØ® ÛŒÚ©Ù… Ú˜Ø§Ù†ÙˆÛŒÙ‡ Û²Û°Û°Û· Ùˆ Ù¾Ø³ Ø§Ø² Ø¢Ù† Ø¨Ù‡ Ø¯Ù†ÛŒØ§ Ø¢Ù…Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø§Ø³Øª.",
  "sources": [
    "Â«Ø®Ø±ÛŒØ¯ØŒ ÙØ±ÙˆØ´ Ùˆ Ù…ØµØ±Ù Ù‡Ø±Ú¯ÙˆÙ†Ù‡ Ù…Ø­ØµÙˆÙ„ Ø¯Ø®Ø§Ù†ÛŒØ§Øª ...",
    "Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø¬Ù‡Ø§Ù†ÛŒ Ù…Ù…Ù†ÙˆØ¹ÛŒØª Ù†Ø³Ù„ÛŒ Ø¯Ø®Ø§Ù†ÛŒØ§Øª ...",
    "Ø¨Ù‡â€ŒÚ¯Ø²Ø§Ø±Ø´ Ø¢ÛŒâ€ŒØ§Ùâ€ŒØ§Ù„â€ŒØ³Ø§ÛŒÙ†Ø³ ...",
    "Ù…Ø§Ù„Ø¯ÛŒÙˆ Ø¨Ø§ Ù‚Ø§Ù†ÙˆÙ† Ù…Ù…Ù†ÙˆØ¹ÛŒØª Ù†Ø³Ù„â€ŒÙ…Ø­ÙˆØ± ...",
    "ØªØ§Ø±ÛŒØ®â€ŒØ³Ø§Ø²ÛŒ Ù…Ø§Ù„Ø¯ÛŒÙˆ Ø¯Ø± Ù…Ø¨Ø§Ø±Ø²Ù‡ Ø¨Ø§ Ø¯Ø®Ø§Ù†ÛŒØ§Øª ..."
  ]
}
```

---

# ğŸ” How the RAG System Works

## **1ï¸âƒ£ Ingestion Pipeline**

* Upload file
* Extract text
* Split into chunks
* Compute embeddings
* Extract metadata
* Persist into ChromaDB

## **2ï¸âƒ£ Query Pipeline**

* Embed query
* Retrieve top-k chunks
* Build RAG prompt
* LLM generates grounded answer
* Return answer + metadata sources

---

# ğŸ”§ Setup

## 1. Clone the repo

```bash
git clone <repo-url>
cd <project-folder>
```

## 2. Install dependencies

```bash
pip install -r requirements.txt
```

## 3. Run the API

```bash
python src/main.py
```

Open Swagger docs:

```
http://127.0.0.1:8000/docs
```

---

# ğŸ“¦ Example Documents

The `static/data/` directory contains Persian TXT & PDF files used to test ingestion.

---
